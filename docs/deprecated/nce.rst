****************************
Noise Contrastive Estimation
****************************

Noise contrastive estimation (NCE) replaces the expensive
vocabulary-sized softmax operation at the final layer of language
(?) models with a cheaper sampling based operation, which results in
significant speed-up during training.  To motivate NCE, let us start
with the basic equations for probabilistic models.

To model the probability distribution :math:`p(y)` of a set of objects
:math:`y \in \mathcal{Y}`, we start with a score function
:math:`s_\theta(y):\mathcal{Y}\rightarrow\mathbb{R}` with adjustable
weights :math:`\theta`.  For example log-linear models use
:math:`s_\theta(y)=\theta^T \phi(y)` where :math:`\theta` is a weight
vector and :math:`\phi(y):\mathcal{Y}\rightarrow\mathbb{R}^D` is a
function that maps an element of :math:`\mathcal{Y}` to a vector of
real valued features.  Conditional models use :math:`p(y|x)`,
:math:`s_\theta(x,y)`, and :math:`\phi(x,y)` instead.  We will stick
with non-conditional notation for brevity.

To go from arbitrary valued scores to normalized probability estimates
we use exponentiation and normalization (which, for some reason, is
called the softmax function):

.. math::
   p_\theta(y) = \frac{\exp s_\theta(y)}{\sum_{y'\in \mathcal{Y}} \exp s_\theta(y')}

The maximum likelihood estimate (MLE) training objective is to
maximize the estimated probability of a given training set
:math:`Y=\{y_1,\ldots,y_n\}`.  Assuming the instances in the training
set are selected independently we have :math:`\log p_\theta(Y) =
\sum_{y\in Y} \log p_\theta(y)`.  The contribution of an individual
instance to the objective is:

.. math::
   L_{MLE}(\theta) = \log p_\theta(y) = s_\theta(y) - \log\sum_{y'\in\mathcal{Y}}\exp s_\theta(y')

Stochastic gradient descent (SGD) uses the gradient of this quantity
with respect to the weights :math:`\theta` to find the MLE solution:

.. math::
   \nabla L_{MLE}(\theta) &= \nabla s_\theta(y) - \nabla \log\sum_{y'\in\mathcal{Y}}\exp s_\theta(y') \\
   &= \nabla s_\theta(y) - \frac{\sum_{y'\in\mathcal{Y}}\exp s_\theta(y') \nabla s_\theta(y')}{\sum_{y'\in\mathcal{Y}}\exp s_\theta(y')} \\
   &= \nabla s_\theta(y) - \sum_{y'\in\mathcal{Y}} p_\theta(y') \nabla s_\theta(y')

Note that the second term in the final equation involves a sum over
the whole :math:`\mathcal{Y}` which is typically a computational
nightmare.  In order to avoid this sum, people have come up with all
sorts of tricks.  One simple example is the *perceptron
approximation* (ref?):

.. math::
   \log\sum_{y'\in\mathcal{Y}}\exp s_\theta(y') \approx \max_{y'\in\mathcal{Y}} s_\theta(y') \\
   \nabla L(\theta) \approx \nabla s_\theta(y) - \nabla \max_{y'\in\mathcal{Y}} s_\theta(y')

Here is an example to demonstrate why this makes sense.  Let's say
:math:`\mathcal{Y}` has three elements and their scores,
:math:`s_\theta(y')`, are 10, 20, and 30.  When we exponentiate these
scores we get :math:`e^{10}`, :math:`e^{20}` and :math:`e^{30}`.  Note
that even though 20 and 30 are not all that different,
:math:`e^{30}\approx 10^{13}` is significantly larger than
:math:`e^{20}\approx 5\times 10^9`.  When we add the exponentials,
:math:`e^{10}+e^{20}+e^{30}`, the result will not be significantly
different from :math:`e^{30}`, and thus the result of the final
:math:`\log` will not be that different from 30 (it is
30.000045401... if you are curious) (and that is why
:math:`\log\sum\exp` *should* be called the softmax function).

The perceptron approximation seems to replace the expensive summation
with an equally expensive looking max operation.  Fortunately the max
operation can be performed fast for certain classes of problems.  This
will be the topic of another chapter, for now let's get back to NCE.

NCE takes a different approach to avoid the costly summation.  Instead
of modeling the empirical distribution :math:`p(y)` directly, it
proposes solving the related binary classification problem of
distinguishing samples generated by :math:`p(y)` from samples
generated by a "noise" distribution :math:`q(y)`.  It is common
practice to use a simple uniform distribution or, for language models,
the unigram distribution for :math:`q(y)`.

How is the binary classification problem of deciding :math:`p` vs
:math:`q` related to the original density estimation problem of
modeling :math:`p`?  Assume we generate :math:`k` noise samples
:math:`y_{1\ldots k}\sim q(y)` for each real sample and add them to
our original training data.  We label each real sample with
:math:`d=1` and each noise sample with :math:`d=0` to train a binary
classifier.  The joint probability of the samples and labels in this
new dataset is:

.. math
.. p(y,d) = \left\{ \begin{array}{ll} 
.. \frac{1}{k+1}\, p(y) & \mbox{if $d=1$} \\
.. \frac{k}{k+1}\, q(y) & \mbox{if $d=0$}
.. \end{array} \right.

.. math::
   p(d=1, y) = \frac{1}{k+1}\,p(y) \qquad
   p(d=0, y) = \frac{k}{k+1}\,q(y)

The conditional probability of the label given the sample is:

.. math::
   p(d=1\mid y) = \frac{p(y)}{p(y)+k\,q(y)} \qquad
   p(d=0\mid y) = \frac{k\,q(y)}{p(y)+k\,q(y)}

This means :math:`p(d=1\mid y)` and :math:`p(y)` are related by a
simple algebraic identity.  In fact, if somebody hands us a good
estimate for :math:`p(d=1\mid y)`, we can turn it into an esimate for
:math:`p(y)`:

.. math::
   p(y) = k\, q(y)\, \frac{p(d=1\mid y)}{p(d=0\mid y)}

.. If :math:`p_\omega(d=1\mid y)` is a good estimate for :math:`p(d=1\mid
.. y)`, we can turn this into an estimate for :math:`p(y)`:

.. math ###
.. p_\omega(y) = k\, q(y)\, \frac{p_\omega(d=1\mid y)}{p_\omega(d=0\mid y)} = k\,q(y)\,\exp s_\omega(y)\\

NCE (ref?) suggests training the following binary classifier model for
:math:`p(d=1\mid y)` on the dataset with noise samples.

.. math::
   p_\theta(d=1\mid y) = \frac{\exp s_\theta(y)}{\exp s_\theta(y) + k\,q(y)}

.. math ###
.. p_\omega(d=1 \mid y) = \frac{\exp s_\omega(y)}{1 + \exp s_\omega(y)}

.. p_\theta(d=1\mid y) &= \frac{p_theta(y)}{p_\theta(y)+k\,q(y)} \\
.. &= \frac{(1/Z) \exp s_\theta(y)}{(1/Z) \exp s_\theta(y) + k\,q(y)}

Using this model will give us the following for :math:`p_\theta(y)`:

.. math::
   p_\theta(y) &= k\, q(y)\, \frac{p_\theta(d=1\mid y)}{p_\theta(d=0\mid y)} \\
   &= k\,q(y)\, \frac{\exp s_\theta(y)}{k\,q(y)} \\
   &= \exp s_\theta(y)

which amounts to assuming our costly normalization term
:math:`Z=\sum_{y'\in \mathcal{Y}} \exp s_\theta(y')` is :math:`1` (ref?).

Our new objective is to maximize the conditional probability of the
NCE dataset.  Consider the conditional log probability of a real
sample :math:`y_0` and :math:`k` noise samples :math:`y_1\ldots y_k`:

.. math::
   L_{NCE}(\theta)
   &= \log p_\theta(d=1\mid y_0) + \sum_{i=1}^k \log p_\theta(d=0\mid y_i) \\
   &= s_\theta(y_0) - \log(\exp s_\theta(y_0) + k\,q(y_0)) + 
      \sum_{i=1}^k \log(k\,q(y_i)) - \log(\exp s_\theta(y_i) + k\,q(y_i))

The gradient of the new objective is:

.. math::
   \nabla L_{NCE}(\theta)
   &= \nabla s_\theta(y_0) - \sum_{i=0}^k \nabla \log(\exp s_\theta(y_i) + k\,q(y_i)) \\
   &= \nabla s_\theta(y_0) - \sum_{i=0}^k p_\theta(d=1\mid y_i) \nabla s_\theta(y_i)

In the limit :math:`k\rightarrow\infty` we see that the NCE gradient
approaches the MLE gradient:

.. math::
   \nabla L_{NCE}(\theta)
   &\rightarrow \nabla s_\theta(y_0) - \sum_{y\in\mathcal{Y}} k\, q(y) p_\theta(d=1\mid y) \nabla s_\theta(y) \\
   &= \nabla s_\theta(y_0) - \sum_{y\in\mathcal{Y}} k\, q(y) \frac{\exp s_\theta(y)}{\exp s_\theta(y) + k\,q(y)} \nabla s_\theta(y) \\
   &\rightarrow \nabla s_\theta(y_0) - \sum_{y\in\mathcal{Y}} \exp s_\theta(y) \nabla s_\theta(y) \\
   &= \nabla s_\theta(y_0) - \sum_{y\in\mathcal{Y}} p_\theta(y) \nabla s_\theta(y)

What does this all mean computationally?  Let's compare the operations
of MLE and NCE language models in their final layers.  Say both models
use a :math:`D`-dimensional internal representation.  For the MLE
model, the output is a :math:`V`-dimensional probability vector where
:math:`V` is the vocabulary size.  The forward pass involves
multiplication of the :math:`D`-dimensional internal representation
with a :math:`V \times D` decoding matrix and normalization of the
result, an :math:`O(VD)` operation.

The NCE model, on the other hand, only needs the scores of the correct
word and :math:`K` additional noise sample words during training.
This involves extracting :math:`K+1` rows from the :math:`V \times D`
decoding matrix, multiplying the :math:`D`-dimensional internal
representation with the resulting :math:`(K+1) \times D` matrix and no
normalization, an :math:`O(KD)` operation.  Because :math:`K \ll V`
this results in a large speed-up.

Here is the backward pass for MLE (with subscripts dropped for
brevity):

.. math::
   &p(y) = \frac{\exp s(y)}{\sum_{y'\in\mathcal{Y}}\exp s(y')} \\
   &L = \log p(y) = s(y) - \log\sum_{y'\in\mathcal{Y}}\exp s(y') \\\
   &{\partial L}/{\partial s(y)} = 1 - p(y) \\
   &{\partial L}/{\partial s(y')} = -p(y')

Here is the backward pass for NCE:

.. math::
   &p(d=1\mid y) = \frac{\exp s(y)}{\exp s(y) + k\,q(y)} \qquad p(d=0\mid y) = \frac{k\,q(y)}{\exp s(y) + k\,q(y)} \\
   &L = \log p(d=1\mid y) + \sum_{i=1}^k \log p(d=0\mid y_i) \\
   &L = s(y) - \log(\exp s(y)+k\,q(y)) + \sum_{i=1}^k \log(k\,q(y_i)) - \log(\exp s(y_i)+k\,q(y_i)) \\
   &{\partial L}/{\partial s(y)} = 1 - p(d=1\mid y) \\
   &{\partial L}/{\partial s(y_i)} = -p(d=1\mid y_i)

.. MLE:
.. s  = wx			;; si is the input to the soft layer
.. qi = (exp si) / (Σ exp sj)	;; qi is the output of the soft layer

.. L = Σ pi log qi                        ;; obj function, pi is the gold probability
..   = Σ pi log (qi/Σqj)                  ;; normalization explicit
..   = (Σ pi log qi) - Σ pi log Σ qj
..   = (Σ pi log qi) - log Σ qj

.. ∂L/∂qk = pk/qk - (1/Σ qj)
..        = pk/qk - 1

.. ∂qi/∂sk = [(i=k)(exp si)(Σ exp sj) - (exp si)(exp sk)] / (Σ exp sj)^2
..         = qi ((i=k) - qk)

.. ∂L/∂sk = Σ (∂L/∂qi)(∂qi/∂sk)	;; derivative wrt the input sk
..        = Σ (pi/qi - 1)((i=k) qi - qi qk)
..        = Σ ((i=k) pi - pi qk - (i=k) qi + qi qk)
..        = pk - qk - qk + qk
..        = pk - qk

.. If we have a single correct word w, pw=1 and pi=0 for i!=w
.. L = log qw
.. ∂L/∂qw = 1/qw - 1
.. ∂L/∂qi = 0/qi - 1 = -1
.. ∂L/∂sw = 1 - qw
.. ∂L/∂si = -qi

.. NCE: w is the correct word i!=w are the noise words, qi is the
.. noise probability
..
.. L = sw - log(exp sw + k qw) + Σ log(k qi) - log(exp si + k qi)
.. ∂L/∂sw = 1 - p(d=1|w)
.. ∂L/∂si = 0 - p(d=1|i)
.. p(d=1|j) = exp sj / (exp sj + k qj)


**Research Ideas:**

* Can we use the words in a minibatch as noise samples for each other,
  presumably with an importance sampling correction factor?
* Can we use other models for the binary classification problem?

**References:**

Gutmann, M. U., & Hyvärinen, A. (2012). Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. The Journal of Machine Learning Research, 13(1), 307-361.

Mnih, A., & Teh, Y. W. (2012). A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426.

Dyer, C. (2014). Notes on Noise Contrastive Estimation and Negative Sampling. arXiv preprint arXiv:1410.8251.
